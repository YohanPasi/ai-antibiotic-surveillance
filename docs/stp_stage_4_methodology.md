
# STP Stage 4 Methodology: Model Evaluation & Clinical Validation
**Version**: 1.0.0
**Governance**: M41-M55 (15 Policies)
**Status**: Implemented & Verified

## 1. Overview
Stage 4 transforms predictive models into trusted clinical tools through rigorous, "reviewer-proof" evaluation. It answers: *"Are these predictions stable, calibrated, and clinically safer than the baseline?"*

## 2. Governance Framework (Implemented)

| ID | Policy | Implementation |
| :--- | :--- | :--- |
| **M41** | Frozen Model Loading | `stp_temporal_backtest.py` loads only `active` models from `stp_model_registry`. |
| **M42** | Prospective Testing | Evaluation windows are strictly future ($T+h$) relative to inputs. |
| **M43** | Horizon Specificity | Metrics reported independently for 1-week, 2-week, 4-week horizons. |
| **M44** | Metric Priority | **NPV > Sensitivity > FNR > Brier > AUROC**. Enforced in `stp_metrics.py`. |
| **M45** | Ward Stratification | `stp_ward_stratified_eval.py` breaks down performance by ICU vs Non-ICU. |
| **M46** | Calibration Check | Reliability curves generated by `stp_calibration.py`. |
| **M51** | Baseline Parity | `BacktestEngine` runs Naive/Mean baselines on the **exact same windows** as ML. |
| **M52** | Calibration Impact | `stp_calibration_impact.py` verifies if calibration improves Brier/NPV. |
| **M53** | Error Cost Framing | Errors reported as "Missed Cases per 100 Ward-Weeks" (Clinical Utility). |
| **M54** | Interpretability Stability | `stp_shap_stability.py` tracks Jaccard Index of top features over time. |
| **M55** | External Validity Limit | API responses include "Single-Center Context" disclaimer. |

## 3. Architecture

### 3.1 Database Schema (New Tables)
-   `stp_model_evaluation_runs`: Metadata for backtest events.
-   `stp_model_metrics`: Stores performance (inc. `model_type` for Baselines, `cost` score).
-   `stp_calibration_results`: Binned probabilities (Obs/Pred).
-   `stp_failure_cases`: Audit log of False Negatives with reasons.
-   `stp_shap_stability_metrics`: Consistency scores.

### 3.2 Evaluation Engines (`api/evaluation_engine/`)
-   **Orchestrator**: `stp_temporal_backtest.py` manages the sliding window loop.
-   **Metrics**: `stp_metrics.py` calculates safety-first scores.
-   **Audit**: `stp_failure_analysis.py` logs specific misses.

## 4. Verification
Automated Unit Tests (`tests/test_stp_stage_4.py`) PASSED:
-   **M44/M53**: Confirmed NPV calculation and Clinical Cost logic.
-   **M52**: Confirmed Calibration Impact logic (Brier reduction check).
-   **M54**: Confirmed Jaccard Stability logic.
-   **M45**: Confirmed Ward Stratification (ICU/Non-ICU splitting).

## 5. Usage
Execute the full evaluation pipeline:
```bash
python scripts/stp_run_evaluation_stage4.py
```
View results via API:
-   `GET /api/stp/stage4/summary`
-   `GET /api/stp/stage4/calibration`
